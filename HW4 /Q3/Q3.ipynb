{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DB0vv4pBcWu9"
   },
   "source": [
    "# Q3 Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Do not modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GalZFbfhcWvA"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score, GridSearchCV, cross_validate, train_test_split\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import pkg_resources\n",
    "from pkg_resources import DistributionNotFound, VersionConflict\n",
    "from platform import python_version\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jordangewing-mullins/miniconda3/envs/HW4_new/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jordangewing-mullins/miniconda3/envs/HW4_new/lib/python3.8/site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m956.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /Users/jordangewing-mullins/miniconda3/envs/HW4_new/lib/python3.8/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jordangewing-mullins/miniconda3/envs/HW4_new/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.0.3-cp38-cp38-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tests as tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify your Python version and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#function to check setup\n",
    "def check_env_setup():\n",
    "    dependencies = open(\"requirements.txt\").readlines()\n",
    "    try:\n",
    "        pkg_resources.require(dependencies)\n",
    "        print(\"✅ ALL GOOD\")\n",
    "    except DistributionNotFound as e:\n",
    "        print(\"⚠️ Library is missing\")\n",
    "        print(e)\n",
    "    except VersionConflict as e:\n",
    "        print(\"⚠️ Library version conflict\")\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Something went wrong\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Library version conflict\n",
      "(scikit-learn 1.0.2 (/Users/jordangewing-mullins/opt/anaconda3/lib/python3.9/site-packages), Requirement.parse('scikit-learn==0.22.1'))\n"
     ]
    }
   ],
   "source": [
    "# verify the environment setup\n",
    "check_env_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add your Georgia Tech Username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class GaTech():\n",
    "    # Change to your GA Tech Username\n",
    "    # NOT your 9-Digit GTId\n",
    "    def GTusername(self):\n",
    "        gt_username = \"jag31\"\n",
    "        return gt_username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Z1gV3UlcWvD"
   },
   "source": [
    "# Q3.1 Data Import\n",
    "Now for the fun stuff. Let’s import some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9VS44b2kcWvE"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class Data():\n",
    "    \n",
    "    # points [1]\n",
    "    def dataAllocation(self,path):\n",
    "        # TODO: Separate out the x_data and y_data and return each\n",
    "        # args: string path for .csv file\n",
    "        # return: pandas dataframe, pandas series\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "\n",
    "        # Read the data from the CSV file\n",
    "        data = pd.read_csv(path, delimiter='\\t')\n",
    "\n",
    "        # Extract the features (x_data) by excluding the 'y' column\n",
    "        x_data = data.drop(columns=['y'])\n",
    "\n",
    "        # Extract the target variable (y_data)\n",
    "        y_data = data['y']\n",
    "        # ------------------------------- \n",
    "        return x_data,y_data\n",
    "    \n",
    "    # points [1]\n",
    "    def trainSets(self,x_data,y_data):\n",
    "        # TODO: Split 70% of the data into training and 30% into test sets. Call them x_train, x_test, y_train and y_test.\n",
    "        # Use the train_test_split method in sklearn with the parameter 'shuffle' set to true and the 'random_state' set to 614.\n",
    "        # args: pandas dataframe, pandas dataframe\n",
    "        # return: pandas dataframe, pandas dataframe, pandas series, pandas series\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, shuffle=True, random_state=614)\n",
    "\n",
    "        # -------------------------------\n",
    "        return x_train, x_test, y_train, y_test\n",
    "\n",
    "##################################################\n",
    "##### Do not add anything below this line ########\n",
    "tests.dataTest(Data)\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q09V5Ux5cWvI"
   },
   "source": [
    "# Q3.2 Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnHXBF1UcWvJ"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class LinearRegressionModel():\n",
    "    \n",
    "    # points [2]\n",
    "\n",
    "    def linearClassifier(self, x_train, x_test, y_train):\n",
    "        # Create a LinearRegression classifier\n",
    "        model = LinearRegression()\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        # Make predictions on the training and test data\n",
    "        y_predict_train = model.predict(x_train)\n",
    "        y_predict_test = model.predict(x_test)\n",
    "        \n",
    "        return y_predict_train, y_predict_test\n",
    "    \n",
    "    # points [1]\n",
    "    def lgTrainAccuracy(self, y_train, y_predict_train):\n",
    "        # Round the predicted values to 0 or 1\n",
    "        y_predict_train_rounded = [1 if val >= 0.5 else 0 for val in y_predict_train]\n",
    "        \n",
    "        # Calculate the training accuracy using accuracy_score\n",
    "        train_accuracy = accuracy_score(y_train, y_predict_train_rounded)\n",
    "        \n",
    "        return train_accuracy\n",
    "    \n",
    "    # points [1]\n",
    "    def lgTestAccuracy(self, y_test, y_predict_test):\n",
    "        # Round the predicted values to 0 or 1\n",
    "        y_predict_test_rounded = [1 if val >= 0.5 else 0 for val in y_predict_test]\n",
    "        \n",
    "        # Calculate the test accuracy using accuracy_score\n",
    "        test_accuracy = accuracy_score(y_test, y_predict_test_rounded)\n",
    "        \n",
    "        return test_accuracy\n",
    "    \n",
    "##################################################\n",
    "##### Do not add anything below this line ########\n",
    "tests.linearTest(Data,LinearRegressionModel)\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbqnCyHAcWvP"
   },
   "source": [
    "# Q3.3 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTtIFJW7cWvQ"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class RFClassifier():\n",
    "    \n",
    "    # points [2]\n",
    "    def randomForestClassifier(self, x_train, x_test, y_train):\n",
    "        # Create a RandomForestClassifier with a random state of 614\n",
    "        rf_clf = RandomForestClassifier(random_state=614)\n",
    "        \n",
    "        # Train the RandomForestClassifier\n",
    "        rf_clf.fit(x_train, y_train)\n",
    "        \n",
    "        # Make predictions on the training and test data\n",
    "        y_predict_train = rf_clf.predict(x_train)\n",
    "        y_predict_test = rf_clf.predict(x_test)\n",
    "        \n",
    "        return rf_clf, y_predict_train, y_predict_test\n",
    "    \n",
    "    # points [1]\n",
    "    def rfTrainAccuracy(self,y_train,y_predict_train):\n",
    "        # TODO: Return accuracy on the training set using the accuracy_score method.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        train_accuracy = accuracy_score(y_train, y_predict_train)\n",
    "        \n",
    "        # -------------------------------\n",
    "        return train_accuracy\n",
    "    \n",
    "    # points [1]\n",
    "    def rfTestAccuracy(self,y_test,y_predict_test):\n",
    "        # TODO: Return accuracy on the test set using the accuracy_score method.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        test_accuracy = accuracy_score(y_test, y_predict_test)\n",
    "        # -------------------------------\n",
    "        return test_accuracy\n",
    "    \n",
    "# Q3.3.1 Feature Importance\n",
    "    \n",
    "    # points [1]\n",
    "    def rfFeatureImportance(self,rf_clf):\n",
    "        # TODO: Determine the feature importance as evaluated by the Random Forest Classifier.\n",
    "        # args: RandomForestClassifier object\n",
    "        # return: float array\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        feature_importance = rf_clf.feature_importances_\n",
    "        # -------------------------------\n",
    "        return feature_importance\n",
    "    \n",
    "    # points [1]\n",
    "    def sortedRFFeatureImportanceIndicies(self,rf_clf):\n",
    "        # TODO: Sort them in the descending order and return the feature numbers[0 to ...].\n",
    "        #       Hint: There is a direct function available in sklearn to achieve this. Also checkout argsort() function in Python.\n",
    "        # args: RandomForestClassifier object\n",
    "        # return: int array\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        # Get the feature importances from the RandomForestClassifier\n",
    "        feature_importance = rf_clf.feature_importances_\n",
    "        \n",
    "        # Sort the indices in descending order based on feature importances\n",
    "        sorted_indices = feature_importance.argsort()[::-1]\n",
    "        # -------------------------------\n",
    "        return sorted_indices\n",
    "    \n",
    "# Q3.3.2 Hyper-parameter Tuning\n",
    "\n",
    "    # points [2]\n",
    "    def hyperParameterTuning(self, rf_clf, x_train, y_train):\n",
    "        # Define the parameter grid for GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_estimators': [4, 16, 256],\n",
    "            'max_depth': [2, 8, 16]\n",
    "        }\n",
    "\n",
    "        # Create a GridSearchCV object\n",
    "        gscv_rfc = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=3)  # You can adjust the value of 'cv' as needed\n",
    "\n",
    "        # Fit the GridSearchCV on the training data\n",
    "        gscv_rfc.fit(x_train, y_train)\n",
    "        \n",
    "        return gscv_rfc\n",
    "    \n",
    "    # points [1]\n",
    "    def bestParams(self,gscv_rfc):\n",
    "        # TODO: Get the best params, using .best_params_\n",
    "        # args:  GridSearchCV object\n",
    "        # return: parameter dict\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        best_params = gscv_rfc.best_params_\n",
    "        \n",
    "        # -------------------------------\n",
    "        return best_params\n",
    "    \n",
    "    # points [1]\n",
    "    def bestScore(self,gscv_rfc):\n",
    "        # TODO: Get the best score, using .best_score_.\n",
    "        # args: GridSearchCV object\n",
    "        # return: float\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        best_score = gscv_rfc.best_score_\n",
    "        # -------------------------------\n",
    "        return best_score\n",
    "    \n",
    "##################################################\n",
    "##### Do not add anything below this line ########\n",
    "tests.RandomForestTest(Data,RFClassifier)\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BNeOPWIpcWvg"
   },
   "source": [
    "# Q3.4 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9msZXyImcWvh"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class SupportVectorMachine():\n",
    "    \n",
    "# Q3.4.1 Pre-process\n",
    "\n",
    "    # points [1]\n",
    "    def dataPreProcess(self,x_train,x_test):\n",
    "        # TODO: Pre-process the data to standardize it, otherwise the grid search will take much longer.\n",
    "        # args: pandas dataframe, pandas dataframe\n",
    "        # return: pandas dataframe, pandas dataframe\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        # Create a StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Fit and transform the scaler on the training data\n",
    "        scaled_x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "        # Transform the test data using the same scaler\n",
    "        scaled_x_test = scaler.transform(x_test)\n",
    "        \n",
    "        # -------------------------------\n",
    "        return scaled_x_train, scaled_x_test\n",
    "    \n",
    "# Q3.4.2 Classification\n",
    "\n",
    "    # points [1]\n",
    "    def SVCClassifier(self, scaled_x_train, scaled_x_test, y_train):\n",
    "        # Create an SVC classifier with gamma='auto'\n",
    "        svc_clf = SVC(gamma='auto')\n",
    "        \n",
    "        # Train the SVC classifier\n",
    "        svc_clf.fit(scaled_x_train, y_train)\n",
    "        \n",
    "        # Make predictions on the training and test data\n",
    "        y_predict_train = svc_clf.predict(scaled_x_train)\n",
    "        y_predict_test = svc_clf.predict(scaled_x_test)\n",
    "        \n",
    "        return y_predict_train, y_predict_test\n",
    "    \n",
    "    # points [1]\n",
    "    def SVCTrainAccuracy(self,y_train,y_predict_train):\n",
    "        # TODO: Return accuracy on the training set using the accuracy_score method.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float \n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        train_accuracy = accuracy_score(y_train, y_predict_train)\n",
    "        # -------------------------------\n",
    "        return train_accuracy\n",
    "    \n",
    "    # points [1]\n",
    "    def SVCTestAccuracy(self,y_test,y_predict_test):\n",
    "        # TODO: Return accuracy on the test set using the accuracy_score method.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float \n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        test_accuracy = accuracy_score(y_test, y_predict_test)\n",
    "        # -------------------------------\n",
    "        return test_accuracy\n",
    "    \n",
    "# Q3.4.3 Hyper-parameter Tuning\n",
    "    \n",
    "    # points [1]\n",
    "    def SVMBestScore(self, scaled_x_train, y_train):\n",
    "        # Define the parameter grid for GridSearchCV\n",
    "        svm_parameters = {\n",
    "            'kernel': ('linear', 'rbf'),\n",
    "            'C': [0.01, 0.1, 1.0]\n",
    "        }\n",
    "\n",
    "        # Create an SVM classifier\n",
    "        svm_clf = SVC(gamma='auto')\n",
    "\n",
    "        # Create a GridSearchCV object\n",
    "        svm_cv = GridSearchCV(estimator=svm_clf, param_grid=svm_parameters, cv=3, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "        # Fit the GridSearchCV on the training data\n",
    "        svm_cv.fit(scaled_x_train, y_train)\n",
    "\n",
    "        # Get the best mean test score\n",
    "        best_score = svm_cv.best_score_\n",
    "\n",
    "        return svm_cv, best_score\n",
    "    \n",
    "    # points [1]\n",
    "    def SVCClassifierParam(self, svm_cv, scaled_x_train, scaled_x_test, y_train):\n",
    "        # Get the best estimator from GridSearchCV\n",
    "        best_estimator = svm_cv.best_estimator_\n",
    "\n",
    "        # Train the best estimator on the standardized training data\n",
    "        best_estimator.fit(scaled_x_train, y_train)\n",
    "\n",
    "        # Make predictions on the training and test data\n",
    "        y_predict_train = best_estimator.predict(scaled_x_train)\n",
    "        y_predict_test = best_estimator.predict(scaled_x_test)\n",
    "\n",
    "        return y_predict_train, y_predict_test\n",
    "\n",
    "    # points [1]\n",
    "    def svcTrainAccuracy(self,y_train,y_predict_train):\n",
    "        # TODO: Return accuracy (on the training set) using the accuracy_score method.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        train_accuracy = accuracy_score(y_train, y_predict_train)\n",
    "        \n",
    "        # -------------------------------\n",
    "        return train_accuracy\n",
    "\n",
    "    # points [1]\n",
    "    def svcTestAccuracy(self,y_test,y_predict_test):\n",
    "        # TODO: Return accuracy (on the test set) using the accuracy_score method.\n",
    "        # args: pandas series, numpy array\n",
    "        # return: float\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        test_accuracy = accuracy_score(y_test, y_predict_test)\n",
    "        # -------------------------------\n",
    "        return test_accuracy\n",
    "    \n",
    "# Q3.4.4 Cross Validation Results\n",
    "\n",
    "    # points [1]\n",
    "    def SVMRankTestScore(self,svm_cv):\n",
    "        # TODO: Return the rank test score for all hyperparameter values that you obtained in Q3.4.3. The \n",
    "        # GridSearchCV class holds a 'cv_results_' dictionary that should help you report these metrics easily.\n",
    "        # args: GridSearchCV object \n",
    "        # return: int array\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        rank_test_score = svm_cv.cv_results_['rank_test_score']\n",
    "        # -------------------------------\n",
    "        return rank_test_score\n",
    "    \n",
    "    # points [1]\n",
    "    def SVMMeanTestScore(self,svm_cv):\n",
    "        # TODO: Return mean test score for all of hyperparameter values that you obtained in Q3.4.3. The \n",
    "        # GridSearchCV class holds a 'cv_results_' dictionary that should help you report these metrics easily.\n",
    "        # args: GridSearchCV object\n",
    "        # return: float array\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        mean_test_score = svm_cv.cv_results_['mean_test_score']\n",
    "        # -------------------------------\n",
    "        return mean_test_score\n",
    "\n",
    "##################################################\n",
    "##### Do not add anything below this line ########\n",
    "tests.SupportVectorMachineTest(Data,SupportVectorMachine)\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2qDYMjgcWv5"
   },
   "source": [
    "# Q3.5 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-C9BuGsqcWv5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class PCAClassifier():\n",
    "    \n",
    "    # points [2]\n",
    "    def pcaClassifier(self, x_data):\n",
    "        # Create a PCA object with n_components=8 and svd_solver='full'\n",
    "        pca = PCA(n_components=8, svd_solver='full')\n",
    "\n",
    "        # Fit the PCA object to the data\n",
    "        pca.fit(x_data)\n",
    "\n",
    "        return pca\n",
    "    \n",
    "    # points [1]\n",
    "    def pcaExplainedVarianceRatio(self, pca):\n",
    "        # TODO: Return percentage of variance explained by each of the selected components\n",
    "        # args: pca_object\n",
    "        # return: float array\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        explained_variance_ratio = pca.explained_variance_ratio_\n",
    "        \n",
    "        # -------------------------------\n",
    "        return explained_variance_ratio\n",
    "    \n",
    "    # points [1]\n",
    "    def pcaSingularValues(self, pca):\n",
    "        # TODO: Return the singular values corresponding to each of the selected components.\n",
    "        # args: pca_object\n",
    "        # return: float array\n",
    "        # -------------------------------\n",
    "        # ADD CODE HERE\n",
    "        singular_values = pca.singular_values_\n",
    "        # -------------------------------\n",
    "        return singular_values\n",
    "    \n",
    "##################################################\n",
    "##### Do not add anything below this line ########\n",
    "tests.PCATest(Data,PCAClassifier)\n",
    "##################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw4q3.soln.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
