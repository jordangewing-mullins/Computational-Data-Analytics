{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5905a69",
   "metadata": {},
   "source": [
    "# CSE6242 - HW3 - Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09289981",
   "metadata": {},
   "source": [
    "Pyspark Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139318cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import hour, when, col, date_format, to_timestamp, round, coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9e0f8",
   "metadata": {},
   "source": [
    "Initialize PySpark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c18c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/18 00:11:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "sc = pyspark.SparkContext(appName=\"HW3-Q1\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ae314",
   "metadata": {},
   "source": [
    "Define function for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5bbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "def load_data():\n",
    "    df = sqlContext.read.option(\"header\",True) \\\n",
    "     .csv(\"yellow_tripdata_2019-01_short.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52409d",
   "metadata": {},
   "source": [
    "### Q1.a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f6e00",
   "metadata": {},
   "source": [
    "Perform data casting to clean incoming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f801b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "def clean_data(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with all the original columns and specified data types\n",
    "    '''\n",
    "    \n",
    "    # 1. Cast columns to the specified data types\n",
    "    df = df.withColumn(\"passenger_count\", df[\"passenger_count\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"total_amount\", df[\"total_amount\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"tip_amount\", df[\"tip_amount\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"trip_distance\", df[\"trip_distance\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"fare_amount\", df[\"fare_amount\"].cast(FloatType()))\n",
    "\n",
    "    # 2. Convert datetime columns to timestamps\n",
    "    df = df.withColumn(\"tpep_pickup_datetime\", to_timestamp(df[\"tpep_pickup_datetime\"]))\n",
    "    df = df.withColumn(\"tpep_dropoff_datetime\", to_timestamp(df[\"tpep_dropoff_datetime\"]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f565d0",
   "metadata": {},
   "source": [
    "### Q1.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4f712",
   "metadata": {},
   "source": [
    "Find rate per person for based on how many passengers travel between pickup and dropoff locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e115152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def common_pair(df):\n",
    "    # Filter out trips with the same pick-up and drop-off location\n",
    "    df = df.filter(F.col(\"PULocationID\") != F.col(\"DOLocationID\"))\n",
    "\n",
    "    # Group by pickup and drop-off locations, sum passenger counts, and sum total amounts\n",
    "    grouped_df = df.groupBy(\"PULocationID\", \"DOLocationID\").agg(\n",
    "        F.sum(\"passenger_count\").alias(\"passenger_count\"),\n",
    "        F.sum(\"total_amount\").alias(\"total_amount\")\n",
    "    )\n",
    "\n",
    "    # Calculate per_person_rate (total_amount divided by passenger_count)\n",
    "    with_per_person_rate = grouped_df.withColumn(\n",
    "        \"per_person_rate\", F.col(\"total_amount\") / F.col(\"passenger_count\")\n",
    "    )\n",
    "\n",
    "    # Sort by total passengers in descending order\n",
    "    sorted_df = with_per_person_rate.sort(F.col(\"passenger_count\").desc())\n",
    "\n",
    "    # Select the top 10 pairs with the highest total passengers and per_person_rate\n",
    "    window_spec = Window.orderBy(\n",
    "        F.col(\"passenger_count\").desc(),\n",
    "        F.col(\"per_person_rate\").desc()\n",
    "    )\n",
    "    ranked_df = sorted_df.withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "    ranked_df = ranked_df.withColumnRenamed(\"total_passengers\", \"passenger_count\")\n",
    "\n",
    "    top_10_df = ranked_df.filter(F.col(\"rank\") <= 10).select(\n",
    "        \"PULocationID\", \"DOLocationID\", \"passenger_count\", \"per_person_rate\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    return top_10_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127574ab",
   "metadata": {},
   "source": [
    "### Q1.c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8fd27",
   "metadata": {},
   "source": [
    "Find trips which trip distances generate the highest tip percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "376c981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "def distance_with_most_tip(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with the following columns:\n",
    "            - trip_distance\n",
    "            - tip_percent\n",
    "            \n",
    "    trip_percent is the percent of tip out of fare_amount\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Filter the data for trips with fares greater than $2.00 and positive trip distances\n",
    "    df = df.filter((col(\"fare_amount\") > 2.00) & (col(\"trip_distance\") > 0))\n",
    "    \n",
    "    # Calculate the tip percent (tip_amount * 100 / fare_amount)\n",
    "    df = df.withColumn(\"tip_percent\", (col(\"tip_amount\") * 100) / col(\"fare_amount\"))\n",
    "    \n",
    "    # Round trip distances up to the closest mile\n",
    "    df = df.withColumn(\"trip_distance\", round(col(\"trip_distance\")))\n",
    "    \n",
    "    # Group by trip_distance and calculate the average tip_percent\n",
    "    df = df.groupBy(\"trip_distance\").agg({\"tip_percent\": \"avg\"})\n",
    "    \n",
    "    # Sort the result in descending order of tip_percent\n",
    "    df = df.orderBy(col(\"avg(tip_percent)\").desc())\n",
    "\n",
    "    # Limit the result to the top 15 trip distances\n",
    "    df = df.limit(15)\n",
    "    \n",
    "    # Rename the columns\n",
    "    df = df.withColumnRenamed(\"trip_distance\", \"trip_distance\")\n",
    "    df = df.withColumnRenamed(\"avg(tip_percent)\", \"tip_percent\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0172fe6",
   "metadata": {},
   "source": [
    "### Q1.d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613c906",
   "metadata": {},
   "source": [
    "Determine the average speed at different times of day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abff9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, when, date_format\n",
    "\n",
    "def time_with_most_traffic(df):\n",
    "    # Convert the pickup time to hours\n",
    "    df = df.withColumn(\"pickup_hour\", hour(df[\"tpep_pickup_datetime\"]))\n",
    "\n",
    "    # Separate hours into AM and PM\n",
    "    df = df.withColumn(\"time_of_day\", \n",
    "                      when((col(\"pickup_hour\") >= 0) & (col(\"pickup_hour\") < 12), \"AM\")\n",
    "                      .otherwise(\"PM\"))\n",
    "\n",
    "    # Calculate the average speed (trip_distance / trip_time) for each hour\n",
    "    df = df.withColumn(\"trip_time_hours\", (df[\"tpep_dropoff_datetime\"].cast(\"long\") - df[\"tpep_pickup_datetime\"].cast(\"long\")) / 3600)\n",
    "    df = df.groupBy(\"time_of_day\", \"pickup_hour\").agg(\n",
    "        avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "        avg(\"trip_time_hours\").alias(\"avg_trip_time\")\n",
    "    )\n",
    "    df = df.withColumn(\"avg_speed\", col(\"avg_distance\") / col(\"avg_trip_time\"))\n",
    "\n",
    "    # Convert pickup_hour to 12-hour format\n",
    "    df = df.withColumn(\"pickup_hour_12\", date_format(\"tpep_pickup_datetime\", \"h\"))\n",
    "    \n",
    "    # Pivot the data to get AM and PM columns\n",
    "    df = df.groupBy(\"pickup_hour_12\").pivot(\"time_of_day\").agg(avg(\"avg_speed\")).drop(\"pickup_hour_12\")\n",
    "    df = df.withColumnRenamed(\"AM\", \"am_avg_speed\")\n",
    "    df = df.withColumnRenamed(\"PM\", \"pm_avg_speed\")\n",
    "    \n",
    "    # Fill null values with 0\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbd7b9",
   "metadata": {},
   "source": [
    "### The below cells are for you to investigate your solutions and will not be graded\n",
    "## Ensure they are commented out prior to submitting to Gradescope to avoid errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9abefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#df = load_data()\n",
    "#df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfa96f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/17 07:18:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/17 07:18:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/17 07:18:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/17 07:18:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/17 07:18:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/17 07:18:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/17 07:18:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/10/17 07:18:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------------+------------------+\n",
      "|PULocationID|DOLocationID|passenger_count|   per_person_rate|\n",
      "+------------+------------+---------------+------------------+\n",
      "|         239|         238|             62|  4.26274198870505|\n",
      "|         237|         236|             60| 4.482500068346659|\n",
      "|         263|         141|             52|3.4190384974846473|\n",
      "|         161|         236|             42| 5.368571440378825|\n",
      "|         148|          79|             42| 4.711904752822149|\n",
      "|         142|         238|             39|  5.05487182812813|\n",
      "|         141|         236|             37| 4.355675723101641|\n",
      "|         239|         143|             37| 4.252162224537617|\n",
      "|         239|         142|             35| 3.817714350564139|\n",
      "|          79|         170|             34| 6.394705884596881|\n",
      "+------------+------------+---------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# common_pair(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e42b46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|trip_distance|       tip_percent|\n",
      "+-------------+------------------+\n",
      "|          1.0|16.948128912014667|\n",
      "|          0.0|15.421387863221868|\n",
      "|          2.0|15.316307008502697|\n",
      "|         17.0| 15.22190379132884|\n",
      "|          5.0|14.791297045921036|\n",
      "|          3.0|14.467840950136962|\n",
      "|         21.0|14.318693182631236|\n",
      "|         19.0|14.024168214294264|\n",
      "|          9.0| 13.56675764510684|\n",
      "|          4.0|13.548640690629924|\n",
      "|          6.0|13.301329030626446|\n",
      "|          8.0|11.935883845460518|\n",
      "|         23.0|11.666666666666666|\n",
      "|         10.0|11.469710555853517|\n",
      "|         18.0|11.405847876237262|\n",
      "|          7.0|11.104493870718887|\n",
      "|         27.0| 9.615384615384615|\n",
      "|         11.0| 9.463471596929628|\n",
      "|         13.0| 9.455648543587467|\n",
      "|         12.0| 8.520943433239987|\n",
      "+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#distance_with_most_tip(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f558c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_with_most_traffic(df).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
